{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqLLodmPn_fC"
      },
      "source": [
        "# RNN Based molecule generation\n",
        "\n",
        "Laurent Cetinsoy\n",
        "\n",
        "In this hands-on we want to generate molecule formulas for denovo-drug discovery.\n",
        "\n",
        "For that we need to use Generative models. Generative models are models which goes beyond classification or simple regression : they are able to generate data that look like previously seens dataset.\n",
        "\n",
        "There exists a lot of models :\n",
        "\n",
        "- Bayesian models like graphical models\n",
        "- Recurrent models (for sequence generation like texte)\n",
        "- Variational auto encoders\n",
        "- Generative adversarial models\n",
        "- Flow and diffusion models\n",
        "\n",
        "\n",
        "In the hands-on we will start by  trainning a character based RNN to generate smile molecules\n",
        "\n",
        "\n",
        "We want to feed smile representations of molecules to an RNN.\n",
        "The basic idea is we will train it to predict the next smile token of a molecule given the previous one.\n",
        "\n",
        "For instance for the following molecule \"CC(=O)NC1=CC=C(O)C=C1\" will may give to the model\n",
        "\n",
        "X = \"CC(=O)N\"\n",
        "y = C\n",
        "\n",
        "and ask the RNN to learn to predict y given X\n",
        "\n",
        "Like a standard language model !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-oGBh_3n_fK"
      },
      "source": [
        "## RNN Language model\n",
        "\n",
        "\n",
        "A language model is a model which predict the next token of a sequence given the previous ones :\n",
        "\n",
        "$ P(X_t | X_{t-1}, X_{t-2}, ..., X_{t-p})  $\n",
        "\n",
        "\n",
        "This model can be learned with a Recurrent neural network\n",
        "\n",
        "$ y = P(X_t | X_{t-1}, X_{t-2}, ..., X_{t-p}) = RNN_{\\theta} (X_{t-1}, X_{t-2}, ..., X_{t-p})  $\n",
        "\n",
        "\n",
        "In order to train such model you need a corpus of data.\n",
        "\n",
        "\n",
        "\n",
        "There are two main ways to do that : Word level model or character level model\n",
        "\n",
        "For character level models, an interesting resource is : http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjg6vf36n_fM"
      },
      "source": [
        "Explain briefly what is the difference between word based language model and character based language model\n",
        "\n",
        "Word-based models tokenize at the word level, so each token represents a complete word. Vocabulary size is large (thousands of words) but sequences are shorter. They can't generate new words outside the vocabulary.\n",
        "\n",
        "Character-based models work at the character level - each character is a token. Vocabulary is small (~50-100 chars) but sequences are much longer. They can generate any word and handle rare words better, but need more computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backend utilisé : torch\n",
            "GPU dispo : True\n",
            "Carte : NVIDIA GeForce RTX 5070 Ti Laptop GPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leo/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Comme TensorFlow est vraiment pas ouf il faut le dire, je met pytorch qui lui supporte mon gpu rtx 5070 ti :)\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import keras\n",
        "print(\"Backend utilisé :\", keras.backend.backend()) \n",
        "\n",
        "import torch\n",
        "print(\"GPU dispo :\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Carte :\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eKnswV9n_fO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYRexoZYn_fR"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPa5xcS7n_fU"
      },
      "source": [
        "Dowload the following dataset : https://github.com/joeymach/Leveraging-VAE-to-generate-molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o2psW9KJn_fW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already exists\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/joeymach/Leveraging-VAE-to-generate-molecules/master/250k_smiles.csv\"\n",
        "filename = \"250k_smiles.csv\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    print(\"Downloaded\")\n",
        "else:\n",
        "    print(\"Already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83yTXQZn_fY"
      },
      "source": [
        "Import pandas and load the first 1000 lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-bIp2SLin_fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1000 molecules\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('250k_smiles.csv', nrows=1000)\n",
        "print(f\"Loaded {len(df)} molecules\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niUqKhSfn_fg"
      },
      "source": [
        "Display the first rows of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Q2H4uqnn_fj"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>logP</th>\n",
              "      <th>qed</th>\n",
              "      <th>SAS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\\n</td>\n",
              "      <td>5.05060</td>\n",
              "      <td>0.702012</td>\n",
              "      <td>2.084095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\\n</td>\n",
              "      <td>3.11370</td>\n",
              "      <td>0.928975</td>\n",
              "      <td>3.432004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...</td>\n",
              "      <td>4.96778</td>\n",
              "      <td>0.599682</td>\n",
              "      <td>2.470633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...</td>\n",
              "      <td>4.00022</td>\n",
              "      <td>0.690944</td>\n",
              "      <td>2.822753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...</td>\n",
              "      <td>3.60956</td>\n",
              "      <td>0.789027</td>\n",
              "      <td>4.035182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              smiles     logP       qed  \\\n",
              "0          CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\\n  5.05060  0.702012   \n",
              "1     C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\\n  3.11370  0.928975   \n",
              "2  N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...  4.96778  0.599682   \n",
              "3  CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...  4.00022  0.690944   \n",
              "4  N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...  3.60956  0.789027   \n",
              "\n",
              "        SAS  \n",
              "0  2.084095  \n",
              "1  3.432004  \n",
              "2  2.470633  \n",
              "3  2.822753  \n",
              "4  4.035182  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcYo14b1n_fk"
      },
      "source": [
        "## Processing the data\n",
        "\n",
        "We need to do the following things :\n",
        "\n",
        "- convert smile tokens to numbers\n",
        "- build  smile token sequences and corresponding labels pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vneGYQzbn_fm"
      },
      "source": [
        "Compute the biggest smile molecule size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jsdp2Zcnn_fo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length: 106\n"
          ]
        }
      ],
      "source": [
        "max_len = df['smiles'].str.len().max()\n",
        "print(f\"Max length: {max_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBsw-a8_n_fo"
      },
      "source": [
        "\n",
        "Code a function **unic_characters(string)** which return the unic characters in a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cBiQ1tQnn_fq"
      },
      "outputs": [],
      "source": [
        "def unic_characters(string):\n",
        "    return sorted(set(string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOpE4hDnn_fr"
      },
      "source": [
        "Concatenate all smile string of the pandas dataframe and use **unic_characters** to get the unic_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eRgCFxEan_ft"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: 33\n",
            "['\\n', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '6', '7', '=', '@', 'B', 'C', 'F', 'H', 'I', 'N', 'O', 'S', '[', '\\\\', ']', 'c', 'l', 'n', 'o', 'r', 's']\n"
          ]
        }
      ],
      "source": [
        "all_smiles = ''.join(df['smiles'].values)\n",
        "unique_chars = unic_characters(all_smiles)\n",
        "print(f\"Unique characters: {len(unique_chars)}\")\n",
        "print(unique_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTHWStA6n_fu"
      },
      "source": [
        "Code a function **map_char_to_int(unic_chars)** which returns a dictionnary where each char is assigned an int value.\n",
        "Add a character to specify the end of the molecule (like \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mdRDzhKLn_fw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 33\n"
          ]
        }
      ],
      "source": [
        "def map_char_to_int(unic_chars):\n",
        "    char_to_int = {char: i for i, char in enumerate(unic_chars)}\n",
        "    char_to_int['\\n'] = len(unic_chars)\n",
        "    return char_to_int\n",
        "\n",
        "char_to_int = map_char_to_int(unique_chars)\n",
        "print(f\"Vocabulary size: {len(char_to_int)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAKnndq1n_fw"
      },
      "source": [
        "Code a function map_int_to_char(unic_chars) which returns the reverse mapping.\n",
        "\n",
        "If you want you can merge both functions in a class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xP_hnl-Yn_fy"
      },
      "outputs": [],
      "source": [
        "def map_int_to_char(unique_chars):\n",
        "    int_to_char = {i: char for i, char in enumerate(unique_chars)}\n",
        "    int_to_char[len(unique_chars)] = '\\n'\n",
        "    return int_to_char\n",
        "\n",
        "int_to_char = map_int_to_char(unique_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56vElXKJn_fz"
      },
      "source": [
        "For each smile molecule add the ending token to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zdg9OEDbn_f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df['smiles'] = df['smiles'] + '\\n'\n",
        "print(df['smiles'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMu3dgFxn_f2"
      },
      "source": [
        "## Building the dataset\n",
        "\n",
        "Now we will create the dataset so that it has the good share for our Keras LSTM model\n",
        "\n",
        "Remember Keras recurrent models expect a 3D array with shapes (n_examples, seq_len, n_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXiDcOcin_f3"
      },
      "source": [
        "What will be n_features in our case ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IXhGsssn_f4"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF_xMoJ-n_f4"
      },
      "source": [
        "Code a function **build_X_and_y(string, i_char, seq_lenght)** which takes a string, a **seq_length** number and a position.\n",
        "\n",
        "\n",
        "It should create X by by getting all character between i and i + seq_length\n",
        "and create y by getting the character following the X sequence\n",
        "it returns X and y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgTay6RHn_f6"
      },
      "source": [
        "Test your function on the following string \"\" with seq_length = 4 and i = [1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Aba9Ddewn_f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i=1: X='C(=O', y=')'\n",
            "i=2: X='(=O)', y='N'\n",
            "i=3: X='=O)N', y='C'\n"
          ]
        }
      ],
      "source": [
        "def build_X_and_y(string, i_char, seq_length):\n",
        "    X = string[i_char:i_char + seq_length]\n",
        "    y = string[i_char + seq_length]\n",
        "    return X, y\n",
        "\n",
        "test_string = \"CC(=O)NC1=CC=C(O)C=C1\\n\"\n",
        "seq_length = 4\n",
        "\n",
        "for i in [1, 2, 3]:\n",
        "    X, y = build_X_and_y(test_string, i, seq_length)\n",
        "    print(f\"i={i}: X='{X}', y='{y}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O2uPi8an_f8"
      },
      "source": [
        "By using build_X_and_y and map_char_to_int build a list nameed X_train and a list named y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4giYzXYFn_f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 2007 training examples\n"
          ]
        }
      ],
      "source": [
        "seq_length = 50\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for smiles in df['smiles'].values:\n",
        "    for i in range(len(smiles) - seq_length):\n",
        "        X, y = build_X_and_y(smiles, i, seq_length)\n",
        "        X_train.append([char_to_int[char] for char in X])\n",
        "        y_train.append(char_to_int[y])\n",
        "\n",
        "print(f\"Created {len(X_train)} training examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi4tdpIBn_f-"
      },
      "source": [
        "Create numpy arrays from the lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XAFHPn1rn_f_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (2007, 50)\n",
            "y_train shape: (2007,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdaI9eSln_gA"
      },
      "source": [
        "Reshape the X numpy array (n_examples, seq_lenght, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gSEDTFwpn_gB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape after reshape: (2007, 50, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "print(f\"X_train shape after reshape: {X_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOQpnQsnn_gB"
      },
      "source": [
        "Normalize X by dividing each values by the total number of unic characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZX4Zzb9nn_gC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train normalized, range: [0.029, 0.971]\n",
            "n_chars: 34, y_train range: [2, 33]\n"
          ]
        }
      ],
      "source": [
        "n_chars = max(char_to_int.values()) + 1\n",
        "X_train = X_train.astype(float) / n_chars\n",
        "print(f\"X_train normalized, range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
        "print(f\"n_chars: {n_chars}, y_train range: [{y_train.min()}, {y_train.max()}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx8SxQvGn_gD"
      },
      "source": [
        "Import Keras and build (at least) a two layered LSTM network with 128 neurone in each.\n",
        "\n",
        "You can also add Dropoutlayers\n",
        "\n",
        "Do you think you should use the return_sequences = True ? If yes, when ?\n",
        "\n",
        "\n",
        "Add a Dense layer on top with with the appropriate activation function and number of neurones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Ejns94Fn_gE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leo/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">264,192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,738</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m264,192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)             │         \u001b[38;5;34m8,738\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,930</span> (1.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m272,930\u001b[0m (1.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,930</span> (1.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m272,930\u001b[0m (1.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "n_classes = int(np.max(y_train) + 1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.LSTM(256, input_shape=(seq_length, 1)), \n",
        "    layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKAYA5u5n_gG"
      },
      "source": [
        "Compile the model with the appropriate loss function and the adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dvtymnQ8n_gH"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=n_classes)\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer=Adam(learning_rate=0.002, clipnorm=1.0), \n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOObIiaon_gI"
      },
      "source": [
        "Train the model on 20 epochs and 10 examples (yeah you read correctly) and check that the model overfits !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FvbUk2MOn_gK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.0000e+00 - loss: 3.5301\n",
            "Epoch 2/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 3.4107\n",
            "Epoch 3/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.3000 - loss: 3.1962\n",
            "Epoch 4/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 2.2238\n",
            "Epoch 5/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 2.1026\n",
            "Epoch 6/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2000 - loss: 1.8613\n",
            "Epoch 7/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2000 - loss: 1.8444\n",
            "Epoch 8/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2000 - loss: 1.7872\n",
            "Epoch 9/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.7605\n",
            "Epoch 10/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.7718\n",
            "Epoch 11/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.7573\n",
            "Epoch 12/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.7337\n",
            "Epoch 13/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2000 - loss: 1.7232\n",
            "Epoch 14/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2000 - loss: 1.7177\n",
            "Epoch 15/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.7187\n",
            "Epoch 16/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.7228\n",
            "Epoch 17/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.7226\n",
            "Epoch 18/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.7172\n",
            "Epoch 19/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.7074\n",
            "Epoch 20/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6994\n",
            "Epoch 21/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.7008\n",
            "Epoch 22/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.7082\n",
            "Epoch 23/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3000 - loss: 1.7103\n",
            "Epoch 24/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.7047\n",
            "Epoch 25/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6982\n",
            "Epoch 26/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6962\n",
            "Epoch 27/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6973\n",
            "Epoch 28/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6985\n",
            "Epoch 29/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6993\n",
            "Epoch 30/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6990\n",
            "Epoch 31/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6969\n",
            "Epoch 32/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6941\n",
            "Epoch 33/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6926\n",
            "Epoch 34/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6931\n",
            "Epoch 35/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6942\n",
            "Epoch 36/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6944\n",
            "Epoch 37/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6932\n",
            "Epoch 38/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6916\n",
            "Epoch 39/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6904\n",
            "Epoch 40/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6902\n",
            "Epoch 41/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6905\n",
            "Epoch 42/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6906\n",
            "Epoch 43/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6901\n",
            "Epoch 44/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6892\n",
            "Epoch 45/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6883\n",
            "Epoch 46/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3000 - loss: 1.6877\n",
            "Epoch 47/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3000 - loss: 1.6874\n",
            "Epoch 48/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3000 - loss: 1.6872\n",
            "Epoch 49/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6871\n",
            "Epoch 50/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6865\n",
            "Epoch 51/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3000 - loss: 1.6856\n",
            "Epoch 52/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3000 - loss: 1.6848\n",
            "Epoch 53/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6846\n",
            "Epoch 54/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3000 - loss: 1.6844\n",
            "Epoch 55/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3000 - loss: 1.6839\n",
            "Epoch 56/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3000 - loss: 1.6832\n",
            "Epoch 57/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6824\n",
            "Epoch 58/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6820\n",
            "Epoch 59/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6814\n",
            "Epoch 60/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6809\n",
            "Epoch 61/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6804\n",
            "Epoch 62/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6797\n",
            "Epoch 63/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6789\n",
            "Epoch 64/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6783\n",
            "Epoch 65/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6777\n",
            "Epoch 66/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6770\n",
            "Epoch 67/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6762\n",
            "Epoch 68/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6755\n",
            "Epoch 69/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6747\n",
            "Epoch 70/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6738\n",
            "Epoch 71/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3000 - loss: 1.6730\n",
            "Epoch 72/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6722\n",
            "Epoch 73/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6712\n",
            "Epoch 74/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6703\n",
            "Epoch 75/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6693\n",
            "Epoch 76/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6683\n",
            "Epoch 77/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6673\n",
            "Epoch 78/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6663\n",
            "Epoch 79/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6652\n",
            "Epoch 80/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3000 - loss: 1.6640\n",
            "Epoch 81/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6629\n",
            "Epoch 82/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6617\n",
            "Epoch 83/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6604\n",
            "Epoch 84/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6591\n",
            "Epoch 85/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6577\n",
            "Epoch 86/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6563\n",
            "Epoch 87/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6551\n",
            "Epoch 88/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6541\n",
            "Epoch 89/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6529\n",
            "Epoch 90/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6505\n",
            "Epoch 91/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6483\n",
            "Epoch 92/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6472\n",
            "Epoch 93/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3000 - loss: 1.6452\n",
            "Epoch 94/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6427\n",
            "Epoch 95/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6411\n",
            "Epoch 96/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6392\n",
            "Epoch 97/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6365\n",
            "Epoch 98/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6339\n",
            "Epoch 99/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6317\n",
            "Epoch 100/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6293\n",
            "Epoch 101/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6263\n",
            "Epoch 102/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6227\n",
            "Epoch 103/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6193\n",
            "Epoch 104/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6159\n",
            "Epoch 105/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3000 - loss: 1.6122\n",
            "Epoch 106/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6078\n",
            "Epoch 107/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.6027\n",
            "Epoch 108/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3000 - loss: 1.5972\n",
            "Epoch 109/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.5914\n",
            "Epoch 110/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.5849\n",
            "Epoch 111/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4000 - loss: 1.5775\n",
            "Epoch 112/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.5687\n",
            "Epoch 113/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3000 - loss: 1.5582\n",
            "Epoch 114/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4000 - loss: 1.5459\n",
            "Epoch 115/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4000 - loss: 1.5315\n",
            "Epoch 116/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 1.5146\n",
            "Epoch 117/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4000 - loss: 1.4941\n",
            "Epoch 118/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 1.4701\n",
            "Epoch 119/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6000 - loss: 1.4438\n",
            "Epoch 120/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 1.4181\n",
            "Epoch 121/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7000 - loss: 1.3976\n",
            "Epoch 122/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6000 - loss: 1.3754\n",
            "Epoch 123/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6000 - loss: 1.3396\n",
            "Epoch 124/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6000 - loss: 1.2461\n",
            "Epoch 125/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6000 - loss: 1.2351\n",
            "Epoch 126/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6000 - loss: 1.3201\n",
            "Epoch 127/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 1.2661\n",
            "Epoch 128/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 1.2050\n",
            "Epoch 129/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 1.1903\n",
            "Epoch 130/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 1.2299\n",
            "Epoch 131/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6000 - loss: 1.1929\n",
            "Epoch 132/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 1.0673\n",
            "Epoch 133/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2000 - loss: 1.3997\n",
            "Epoch 134/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7000 - loss: 1.2072\n",
            "Epoch 135/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 1.1167\n",
            "Epoch 136/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6000 - loss: 1.1154\n",
            "Epoch 137/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7000 - loss: 1.0140\n",
            "Epoch 138/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4000 - loss: 1.2785\n",
            "Epoch 139/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 1.1994\n",
            "Epoch 140/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.9666\n",
            "Epoch 141/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.9897\n",
            "Epoch 142/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.9183\n",
            "Epoch 143/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 1.1839\n",
            "Epoch 144/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7000 - loss: 1.1189\n",
            "Epoch 145/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7000 - loss: 0.9666\n",
            "Epoch 146/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7000 - loss: 0.9996\n",
            "Epoch 147/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 0.9683\n",
            "Epoch 148/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.9478\n",
            "Epoch 149/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.8926\n",
            "Epoch 150/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.8131\n",
            "Epoch 151/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.8004\n",
            "Epoch 152/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 1.0310\n",
            "Epoch 153/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.9291\n",
            "Epoch 154/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.8206\n",
            "Epoch 155/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.8425\n",
            "Epoch 156/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8000 - loss: 0.7386\n",
            "Epoch 157/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6000 - loss: 0.9333\n",
            "Epoch 158/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6000 - loss: 0.9528\n",
            "Epoch 159/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8000 - loss: 0.7963\n",
            "Epoch 160/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.6968\n",
            "Epoch 161/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8000 - loss: 0.8672\n",
            "Epoch 162/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8000 - loss: 0.8449\n",
            "Epoch 163/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9000 - loss: 0.7714\n",
            "Epoch 164/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7000 - loss: 0.7641\n",
            "Epoch 165/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.6614\n",
            "Epoch 166/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 1.1267\n",
            "Epoch 167/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4000 - loss: 1.2681\n",
            "Epoch 168/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7000 - loss: 0.6913\n",
            "Epoch 169/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7000 - loss: 0.7750\n",
            "Epoch 170/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6000 - loss: 0.8794\n",
            "Epoch 171/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.7886\n",
            "Epoch 172/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7000 - loss: 0.7196\n",
            "Epoch 173/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.8052\n",
            "Epoch 174/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8000 - loss: 0.7481\n",
            "Epoch 175/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7000 - loss: 0.5873\n",
            "Epoch 176/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 0.9455\n",
            "Epoch 177/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6000 - loss: 1.1946\n",
            "Epoch 178/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6000 - loss: 0.9091\n",
            "Epoch 179/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9000 - loss: 0.5453\n",
            "Epoch 180/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 1.0531\n",
            "Epoch 181/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3000 - loss: 1.2993\n",
            "Epoch 182/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6000 - loss: 0.8753\n",
            "Epoch 183/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.7493\n",
            "Epoch 184/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7000 - loss: 0.7764\n",
            "Epoch 185/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9000 - loss: 0.7516\n",
            "Epoch 186/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.7072\n",
            "Epoch 187/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7000 - loss: 0.6911\n",
            "Epoch 188/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7000 - loss: 0.6830\n",
            "Epoch 189/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7000 - loss: 0.6682\n",
            "Epoch 190/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7000 - loss: 0.6198\n",
            "Epoch 191/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8000 - loss: 0.6415\n",
            "Epoch 192/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8000 - loss: 0.6424\n",
            "Epoch 193/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - loss: 0.5502\n",
            "Epoch 194/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.7049\n",
            "Epoch 195/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.8678\n",
            "Epoch 196/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7000 - loss: 0.6850\n",
            "Epoch 197/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9000 - loss: 0.5137\n",
            "Epoch 198/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9000 - loss: 0.5261\n",
            "Epoch 199/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.5113\n",
            "Epoch 200/200\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8000 - loss: 0.4726\n"
          ]
        }
      ],
      "source": [
        "X_small = X_train[:10]\n",
        "y_small = y_train_cat[:10]\n",
        "\n",
        "history = model.fit(X_small, y_small, epochs=200, batch_size=10, verbose=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMED_5jJn_gL"
      },
      "source": [
        "If it does not overfit try to fix data prep and model architecture so it does"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ps0V-QBjn_gM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it does overfit\n"
          ]
        }
      ],
      "source": [
        "print(\"it does overfit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE2aGAFdn_gN"
      },
      "source": [
        "Create a function **make_prediction(seed_start)** which takes a starting string sequence and uses it to generate a molecule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XrYsgfQbn_gO"
      },
      "outputs": [],
      "source": [
        "def make_prediction(seed_start, model, n_chars, char_to_int, int_to_char, seq_length=50, max_length=100):\n",
        "    molecule = seed_start\n",
        "    seed = seed_start[-seq_length:] if len(seed_start) >= seq_length else seed_start\n",
        "    \n",
        "    pad_char = list(char_to_int.keys())[0]\n",
        "    for _ in range(max_length):\n",
        "        if len(seed) < seq_length:\n",
        "            current_seed = pad_char * (seq_length - len(seed)) + seed\n",
        "        else:\n",
        "            current_seed = seed[-seq_length:]\n",
        "        \n",
        "        x = np.array([[char_to_int[char] for char in current_seed]])\n",
        "        x = x.reshape((1, seq_length, 1))\n",
        "        x = x.astype(float) / n_chars\n",
        "        \n",
        "        pred = model.predict(x, verbose=0)\n",
        "        pred_idx = np.argmax(pred[0])\n",
        "        next_char = int_to_char[pred_idx]\n",
        "        \n",
        "        if next_char == '\\n':\n",
        "            break\n",
        "            \n",
        "        molecule += next_char\n",
        "        seed += next_char\n",
        "    \n",
        "    return molecule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSQFCZQ2n_gP"
      },
      "source": [
        "generate a molecule of your overfitted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2Rce6PKHn_gQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated molecule: CC(=O)NccC3cc)C3ccC\n"
          ]
        }
      ],
      "source": [
        "seed = \"CC(=O)N\"\n",
        "generated = make_prediction(seed, model, n_chars, char_to_int, int_to_char)\n",
        "print(f\"Generated molecule: {generated}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVbrT5zOn_gR"
      },
      "source": [
        "Make a model checkpoint so that the model is saved after each epoch\n",
        "if you train on a plateform and it stops you do not lose your training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UYiShXWrn_gS"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model.keras', monitor='loss', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXA2p4Qn_gT"
      },
      "source": [
        "Now go to your favorite plateform (colab or something else) and train the dataset on the whole data for 10 epochs and batch size 256\n",
        "\n",
        "it should take a long time so either follow the class or go take a nap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_full = pd.read_csv('250k_smiles.csv')\n",
        "df_full['smiles'] = df_full['smiles'] + '\\n'\n",
        "\n",
        "all_smiles_full = ''.join(df_full['smiles'].values)\n",
        "unique_chars_full = unic_characters(all_smiles_full)\n",
        "char_to_int_full = map_char_to_int(unique_chars_full)\n",
        "int_to_char_full = map_int_to_char(unique_chars_full)\n",
        "\n",
        "X_train_full = []\n",
        "y_train_full = []\n",
        "\n",
        "for smiles in df_full['smiles'].values:\n",
        "    for i in range(len(smiles) - seq_length):\n",
        "        X, y = build_X_and_y(smiles, i, seq_length)\n",
        "        X_train_full.append([char_to_int_full[char] for char in X])\n",
        "        y_train_full.append(char_to_int_full[y])\n",
        "\n",
        "X_train_full = np.array(X_train_full).reshape((len(X_train_full), seq_length, 1))\n",
        "n_chars_full = max(char_to_int_full.values()) + 1\n",
        "X_train_full = X_train_full.astype(float) / n_chars_full\n",
        "y_train_full_cat = to_categorical(y_train_full, num_classes=n_chars_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A0av8BLun_gU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_model.keras found, loading existing model.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('best_model.keras'):\n",
        "    print(\"best_model.keras found, loading existing model.\")\n",
        "    model_full = keras.models.load_model('best_model.keras')\n",
        "else:\n",
        "    print(\"No saved model found. Training new model...\")\n",
        "    model_full = keras.Sequential([\n",
        "        layers.LSTM(128, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(128, return_sequences=False),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(n_chars_full, activation='softmax')\n",
        "    ])\n",
        "    model_full.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    checkpoint = ModelCheckpoint('best_model.keras', monitor='loss', save_best_only=True, verbose=1)\n",
        "    model_full.fit(X_train_full, y_train_full_cat, epochs=10, batch_size=256, callbacks=[checkpoint], verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8p3dNjqn_gV"
      },
      "source": [
        "Generate between 100 and 1000 molecules.\n",
        "\n",
        "create a list where molecules have between 10 and 50 atoms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4bIsQ95qn_gW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Lancement de la génération par amorçage (Priming) ---\n",
            "Trouvé 5/100...\n",
            "Trouvé 10/100...\n",
            "Trouvé 15/100...\n",
            "\n",
            "Fini ! 18 molécules valides.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from rdkit import Chem, RDLogger\n",
        "\n",
        "RDLogger.DisableLog('rdApp.*') \n",
        "\n",
        "def sample(preds, temperature=0.4):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-7) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "seeds = df_full['smiles'].sample(200).values\n",
        "seeds = [s[:15] for s in seeds]\n",
        "\n",
        "generated_molecules = []\n",
        "target_n = 100\n",
        "tries = 0\n",
        "\n",
        "print(\"--- Lancement de la génération par amorçage (Priming) ---\")\n",
        "\n",
        "while len(generated_molecules) < target_n and tries < len(seeds):\n",
        "    seed_str = seeds[tries]\n",
        "    tries += 1\n",
        "    \n",
        "    generated = seed_str\n",
        "    sentence = seed_str\n",
        "    \n",
        "    for _ in range(60):\n",
        "        x_int = [char_to_int_full.get(c, 0) for c in sentence[-seq_length:]]\n",
        "        if len(x_int) < seq_length:\n",
        "            x_int = [0] * (seq_length - len(x_int)) + x_int\n",
        "            \n",
        "        x_input = np.array(x_int).reshape(1, seq_length, 1)\n",
        "        x_input = x_input.astype(float) / n_chars_full\n",
        "        \n",
        "        preds = model_full.predict(x_input, verbose=0)[0]\n",
        "        next_char = int_to_char_full[sample(preds, temperature=0.5)]\n",
        "        \n",
        "        if next_char == '\\n':\n",
        "            break\n",
        "            \n",
        "        generated += next_char\n",
        "        sentence += next_char\n",
        "\n",
        "    mol = Chem.MolFromSmiles(generated)\n",
        "    if mol is not None:\n",
        "        atom_count = mol.GetNumAtoms()\n",
        "        if 10 <= atom_count <= 50:\n",
        "            generated_molecules.append(generated)\n",
        "            if len(generated_molecules) % 5 == 0:\n",
        "                print(f\"Trouvé {len(generated_molecules)}/{target_n}...\")\n",
        "\n",
        "print(f\"\\nFini ! {len(generated_molecules)} molécules valides.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDn_LM_ln_gW"
      },
      "source": [
        "With rdkit compute the Quantified Estimated Drug likelyness (QED) of each molecule in this subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TCEPaVUMn_gW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid molecules: 18/18\n",
            "Average QED: 0.670\n",
            "Best QED: 0.833\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import QED\n",
        "\n",
        "qed_scores = []\n",
        "valid_molecules = []\n",
        "\n",
        "for mol_str in generated_molecules:\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(mol_str)\n",
        "        if mol is not None:\n",
        "            qed = QED.qed(mol)\n",
        "            qed_scores.append(qed)\n",
        "            valid_molecules.append(mol_str)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"Valid molecules: {len(valid_molecules)}/{len(generated_molecules)}\")\n",
        "print(f\"Average QED: {np.mean(qed_scores):.3f}\")\n",
        "print(f\"Best QED: {np.max(qed_scores):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGxLKEonn_gX"
      },
      "source": [
        "Bonus 1 : Using rdkit, compute the quantitative estimation of drug-likeness (QED) of your generated molecules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iY9gpNjgn_gY"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QED statistics:\n",
            "Mean: 0.670\n",
            "Std: 0.092\n",
            "Min: 0.446\n",
            "Max: 0.833\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(qed_scores, bins=30, edgecolor='black')\n",
        "plt.xlabel('QED Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of QED Scores for Generated Molecules')\n",
        "plt.show()\n",
        "\n",
        "print(f\"QED statistics:\")\n",
        "print(f\"Mean: {np.mean(qed_scores):.3f}\")\n",
        "print(f\"Std: {np.std(qed_scores):.3f}\")\n",
        "print(f\"Min: {np.min(qed_scores):.3f}\")\n",
        "print(f\"Max: {np.max(qed_scores):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S07WE9mJn_gZ"
      },
      "source": [
        "Bonus 2 : try to adapt a transformer model training from hugging face to see if it is better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Xp_QpW2hn_ga"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'pad'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     20\u001b[39m args = TrainingArguments(\n\u001b[32m     21\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mtransformer_output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     per_device_train_batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     use_cpu=\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Utilise ton GPU\u001b[39;00m\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     model=model_hf,\n\u001b[32m     32\u001b[39m     args=args,\n\u001b[32m     33\u001b[39m     train_dataset=dataset,\n\u001b[32m     34\u001b[39m     data_collator=DataCollatorForLanguageModeling(tokenizer=\u001b[38;5;28;01mNone\u001b[39;00m, mlm=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/transformers/data/data_collator.py:45\u001b[39m, in \u001b[36mDataCollatorMixin.__call__\u001b[39m\u001b[34m(self, features, return_tensors)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tf_call(features)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy_call(features)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/transformers/data/data_collator.py:1033\u001b[39m, in \u001b[36mDataCollatorForLanguageModeling.torch_call\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_rng()\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[32m0\u001b[39m], Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     batch = {\n\u001b[32m   1038\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m.tokenizer, pad_to_multiple_of=\u001b[38;5;28mself\u001b[39m.pad_to_multiple_of)\n\u001b[32m   1039\u001b[39m     }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/transformers/data/data_collator.py:59\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[33m\"\u001b[39m\u001b[33mdeprecation_warnings\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m(*pad_args, **pad_kwargs)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[32m     62\u001b[39m warning_state = tokenizer.deprecation_warnings.get(\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'pad'"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# 1. Config du modèle (petit GPT-2 custom)\n",
        "config = GPT2Config(\n",
        "    vocab_size=len(char_to_int) + 1, # +1 pour le padding\n",
        "    n_positions=128,\n",
        "    n_ctx=128,\n",
        "    n_embd=128,\n",
        "    n_layer=2,\n",
        "    n_head=4\n",
        ")\n",
        "model_hf = GPT2LMHeadModel(config)\n",
        "\n",
        "# 2. Prépa des données \"à la main\" pour aller vite\n",
        "# On transforme nos séquences d'entiers en dataset HuggingFace\n",
        "dataset = Dataset.from_dict({\"input_ids\": X_train.squeeze().astype(int)})\n",
        "\n",
        "# 3. Training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"transformer_output\",\n",
        "    per_device_train_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-3,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    use_cpu=False # Utilise ton GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_hf,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=None, mlm=False)\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
