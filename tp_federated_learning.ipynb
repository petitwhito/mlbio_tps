{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load FashionMNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Simple CNN & Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check epoch 0, loss: 2.258\n",
            "Check epoch 1, loss: 2.191\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(1024, 512) \n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 1024) \n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Quick check on subset\n",
        "subset = Subset(train_dataset, range(200))\n",
        "loader = DataLoader(subset, batch_size=10)\n",
        "model = Net().to(device)\n",
        "opt = optim.SGD(model.parameters(), lr=0.01)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(2):\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = crit(out, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Check epoch {epoch}, loss: {loss.item():.3f}\")\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Average function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "def average_model_parameters(models, weights):\n",
        "    avg_params = copy.deepcopy(models[0].state_dict())\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for key in avg_params:\n",
        "            avg_params[key] = avg_params[key] * weights[0]\n",
        "            \n",
        "        for i in range(1, len(models)):\n",
        "            params = models[i].state_dict()\n",
        "            for key in avg_params:\n",
        "                avg_params[key] += params[key] * weights[i]\n",
        "                \n",
        "    return avg_params\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Algo 1 helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "    \n",
        "def client_update(model, train_loader, epochs=1):\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for _ in range(epochs):\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Independent training (Fail case)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net1 Acc: 0.700\n",
            "Net2 Acc: 0.624\n",
            "Avg Model Acc (Indep Init): 0.395\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "idx = list(range(len(train_dataset)))\n",
        "# 2 clients, 600 pts each\n",
        "ds1 = Subset(train_dataset, idx[0:600])\n",
        "ds2 = Subset(train_dataset, idx[600:1200])\n",
        "ld1 = DataLoader(ds1, batch_size=50, shuffle=True)\n",
        "ld2 = DataLoader(ds2, batch_size=50, shuffle=True)\n",
        "\n",
        "# Diff init\n",
        "net1 = Net().to(device)\n",
        "net2 = Net().to(device)\n",
        "\n",
        "# Train\n",
        "net1 = client_update(net1, ld1, epochs=5)\n",
        "net2 = client_update(net2, ld2, epochs=5)\n",
        "\n",
        "print(f\"Net1 Acc: {test(net1, test_loader):.3f}\")\n",
        "print(f\"Net2 Acc: {test(net2, test_loader):.3f}\")\n",
        "\n",
        "# Avg\n",
        "avg_state = average_model_parameters([net1, net2], [0.5, 0.5])\n",
        "global_net = Net().to(device)\n",
        "global_net.load_state_dict(avg_state)\n",
        "\n",
        "print(f\"Avg Model Acc (Indep Init): {test(global_net, test_loader):.3f}\")\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why it does not work:\n",
        "The averaging fails because the two models were initialized randomly and independently. In deep learning, the loss landscape is non-convex. Since net1 and net2 started from different points, they converged to different local minima. Averaging their parameters results in a point in the parameter space that lies somewhere between these two minima, which typically corresponds to a region with high loss (a \"bad\" model). As stated in the paper, we need a common initialization to ensure the models stay in the same basin of attraction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Common Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg Model Acc (Common Init): 0.672\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "# Same split as before\n",
        "global_init = Net().to(device)\n",
        "init_state = global_init.state_dict()\n",
        "\n",
        "net1 = Net().to(device) \n",
        "net1.load_state_dict(copy.deepcopy(init_state))\n",
        "net2 = Net().to(device)\n",
        "net2.load_state_dict(copy.deepcopy(init_state))\n",
        "\n",
        "net1 = client_update(net1, ld1, epochs=5)\n",
        "net2 = client_update(net2, ld2, epochs=5)\n",
        "\n",
        "avg_state = average_model_parameters([net1, net2], [0.5, 0.5])\n",
        "global_init.load_state_dict(avg_state)\n",
        "\n",
        "print(f\"Avg Model Acc (Common Init): {test(global_init, test_loader):.3f}\")\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. & 8. Study: Data points vs Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running study...\n",
            "M=2, P=25 -> 0.610\n",
            "M=2, P=50 -> 0.658\n",
            "M=2, P=100 -> 0.690\n",
            "M=2, P=200 -> 0.733\n",
            "M=2, P=500 -> 0.769\n",
            "M=3, P=25 -> 0.571\n",
            "M=3, P=50 -> 0.655\n",
            "M=3, P=100 -> 0.701\n",
            "M=3, P=200 -> 0.757\n",
            "M=3, P=500 -> 0.819\n",
            "M=5, P=25 -> 0.591\n",
            "M=5, P=50 -> 0.650\n",
            "M=5, P=100 -> 0.721\n",
            "M=5, P=200 -> 0.754\n",
            "M=5, P=500 -> 0.823\n",
            "\n",
            "\n",
            "models       2       3       5\n",
            "points                        \n",
            "25      0.6095  0.5714  0.5913\n",
            "50      0.6580  0.6548  0.6501\n",
            "100     0.6902  0.7014  0.7212\n",
            "200     0.7328  0.7572  0.7540\n",
            "500     0.7695  0.8185  0.8228\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "def run_fed_avg(n_models, pts_per_model, rounds=5):\n",
        "    all_idx = np.random.permutation(len(train_dataset))\n",
        "    \n",
        "    loaders = []\n",
        "    for i in range(n_models):\n",
        "        subset_idx = all_idx[i*pts_per_model : (i+1)*pts_per_model]\n",
        "        # batch size 50 or full batch if small data\n",
        "        bs = min(50, pts_per_model)\n",
        "        loaders.append(DataLoader(Subset(train_dataset, subset_idx), batch_size=bs, shuffle=True))\n",
        "        \n",
        "    global_model = Net().to(device)\n",
        "    \n",
        "    for r in range(rounds):\n",
        "        w_global = global_model.state_dict()\n",
        "        local_models = []\n",
        "        \n",
        "        for k in range(n_models):\n",
        "            m = Net().to(device)\n",
        "            m.load_state_dict(copy.deepcopy(w_global))\n",
        "            m = client_update(m, loaders[k], epochs=5)\n",
        "            local_models.append(m)\n",
        "            \n",
        "        # uniform weights\n",
        "        weights = [1.0/n_models] * n_models\n",
        "        new_state = average_model_parameters(local_models, weights)\n",
        "        global_model.load_state_dict(new_state)\n",
        "        \n",
        "    return test(global_model, test_loader)\n",
        "\n",
        "# Run loops\n",
        "res = []\n",
        "models_list = [2, 3, 5]\n",
        "points_list = [25, 50, 100, 200, 500]\n",
        "\n",
        "print(\"Running study...\")\n",
        "for m in models_list:\n",
        "    for p in points_list:\n",
        "        acc = run_fed_avg(m, p, rounds=10) # 10 rounds to be sure\n",
        "        res.append({'models': m, 'points': p, 'acc': acc})\n",
        "        print(f\"M={m}, P={p} -> {acc:.3f}\")\n",
        "\n",
        "df = pd.DataFrame(res)\n",
        "pivoted = df.pivot(index='points', columns='models', values='acc')\n",
        "print(\"\\n\")\n",
        "print(pivoted)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Repeat on CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading/Loading CIFAR-10...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "/home/leo/Bureau/mlbio_tps/.venv/lib/python3.13/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running CIFAR-10 Study (Takes longer)...\n",
            "CIFAR Models=2, Points=100 -> Acc: 0.100\n",
            "CIFAR Models=2, Points=500 -> Acc: 0.301\n",
            "CIFAR Models=5, Points=100 -> Acc: 0.165\n",
            "CIFAR Models=5, Points=500 -> Acc: 0.314\n",
            "\n",
            "CIFAR Results:\n",
            "models       2       5\n",
            "points                \n",
            "100     0.1001  0.1647\n",
            "500     0.3007  0.3141\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "print(\"Downloading/Loading CIFAR-10...\")\n",
        "\n",
        "transform_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_cifar = datasets.CIFAR10('./data', train=True, download=True, transform=transform_cifar)\n",
        "test_cifar = datasets.CIFAR10('./data', train=False, download=True, transform=transform_cifar)\n",
        "test_loader_cifar = DataLoader(test_cifar, batch_size=1000, shuffle=False)\n",
        "\n",
        "class NetRGB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetRGB, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 512) \n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def run_cifar_study(n_models, pts_per_model, rounds=5):\n",
        "    indices = np.random.permutation(len(train_cifar))\n",
        "    loaders = []\n",
        "    \n",
        "    for i in range(n_models):\n",
        "        idx = indices[i*pts_per_model : (i+1)*pts_per_model]\n",
        "        loaders.append(DataLoader(Subset(train_cifar, idx), batch_size=min(50, pts_per_model), shuffle=True))\n",
        "        \n",
        "    global_net = NetRGB().to(device)\n",
        "    \n",
        "    for r in range(rounds):\n",
        "        w_global = global_net.state_dict()\n",
        "        locals_list = []\n",
        "        \n",
        "        for k in range(n_models):\n",
        "            m = NetRGB().to(device)\n",
        "            m.load_state_dict(copy.deepcopy(w_global))\n",
        "            \n",
        "            m.train()\n",
        "            opt = optim.SGD(m.parameters(), lr=0.01)\n",
        "            crit = nn.CrossEntropyLoss()\n",
        "            \n",
        "            for _ in range(5):\n",
        "                for x, y in loaders[k]:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    opt.zero_grad()\n",
        "                    loss = crit(m(x), y)\n",
        "                    loss.backward()\n",
        "                    opt.step()\n",
        "            locals_list.append(m)\n",
        "        \n",
        "        avg = average_model_parameters(locals_list, [1.0/n_models]*n_models)\n",
        "        global_net.load_state_dict(avg)\n",
        "        \n",
        "    global_net.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader_cifar:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = global_net(x)\n",
        "            pred = out.argmax(1)\n",
        "            correct += pred.eq(y).sum().item()\n",
        "    return correct / len(test_cifar)\n",
        "\n",
        "print(\"\\nRunning CIFAR-10 Study...\")\n",
        "res_cifar = []\n",
        "models_c = [2, 5] \n",
        "points_c = [100, 500] \n",
        "\n",
        "for m in models_c:\n",
        "    for p in points_c:\n",
        "        acc = run_cifar_study(m, p, rounds=10)\n",
        "        res_cifar.append({'models': m, 'points': p, 'acc': acc})\n",
        "        print(f\"CIFAR Models={m}, Points={p} -> Acc: {acc:.3f}\")\n",
        "\n",
        "print(\"\\nCIFAR Results:\")\n",
        "print(pd.DataFrame(res_cifar).pivot(index='points', columns='models', values='acc'))\n",
        "\n",
        "  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
